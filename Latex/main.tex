\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[slovene]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[nottoc]{tocbibind}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{ dsfont }
\usepackage{siunitx}
\usepackage{multimedia}
\usepackage[table,xcdraw]{xcolor}
\setlength\parindent{0pt}

\newcommand{\ddd}{\mathrm{d}}
\newcommand\myworries[1]{\textcolor{red}{#1}}
\newcommand{\Dd}[3][{}]{\frac{\ddd^{#1} #2}{\ddd #3^{#1}}}

\begin{document}
\begin{titlepage}
    \begin{center}
        \includegraphics[]{logo.png}
        \vspace*{3cm}
        
        \Huge
        \textbf{Numerična minimizacija}
        
        \vspace{0.5cm}
        \large
        3. naloga pri Modelski Analizi 1

        \vspace{4.5cm}
        
        \textbf{Avtor:} Marko Urbanč (28232019)\ \\
        \textbf{Predavatelj:} prof. dr. Simon Širca\ \\
        \textbf{Asistent:} doc. dr. Miha Mihovilovič\ \\
        
        \vspace{2.8cm}
        
        \large
        24.2.2023
    \end{center}
\end{titlepage}
\tableofcontents
\newpage
\section{Uvod}
Numerično minimizacijo poznamo tudi pod širšim imenom matematična optimizacija.
V zelo preprostih pojmih gre za izbito najbolj primernega elementa iz neke množice, 
glede na podane kriterije. Običajno je ta množica neka funkcija, ki jo želimo minimizirati.
Kriterij pa je podan z neko funkcijo, ki nam pove kako dober je neki element. Če se to sliši
zelo podobno kot uvod pri prejšnji nalogi, kjer smo si pogledali Linearno programiranje, je to
zato, ker je to v bistvu ista stvar. Razlika je le v tem, da je pri linearnem programiranju
funkcija, ki jo minimiziramo linearna, pri numerični minimizaciji pa je ta funkcija lahko
poljubna. \\

Z največjim veseljem bi napisal še kakšen bolj matematičen uvod, ampak to bi pomenilo, da bi 
se moral spustiti v podrobnosti delovanja posameznih optimizacijskih algoritmov, kar pa je
precej obsežna tema, še sploh če bi se želel dotakniti vseh, ki sem jih poskusil, ker jih je
res veliko. \\

Veliko problemov se prevede na optimizacijske probleme, zato je to zelo pomembna tema. Če 
ne drugega, je dandanes strašno priljubljeno strojno učenje, ki je v bistvu nič drugega kot
optimizacija neke funkcije, ki nam pove kako dobro se nek model prilega podatkom. Tu se pojavi 
poanta, ki sem jo želel (a neuspešno) povedati v prejšnji nalogi. Optimizacija funkcije z npr. 40 
parametri je zelo malo. V praksi smo zmožni optimizirati funkcije z več milijoni parametrov in tu
se vmeša meni priljubljen High Performance Computing in to da sem želel pri prejšnji nalogi 400k
dimenzij. Tukaj sicer ne bomo počeli tega, ne ker ne bi želel, ampak ker žal nisem utegnil. \\

\section{Naloga}
Naloga je sestavljena iz dveh delov. Poglejmo:
\subsection{Thomsonov problem}
Thomsonov problem je problem, kjer želimo najti najboljšo porazdelitev n točk po sferi, tako da
bo potencial (lahko si mislimo kot električni potencial) čim manjši. To je v bistvu problem, ki
ga rešujemo pri modeliranju atomov. Naloga želi, da za različne metode optimizacije preštudiramo
pojav in natančnost rešitev. \\

\subsection{Vožnja skozi semafor}
Vrača se primer, ki smo ga imeli za prvo nalogo pri Modelski Analizi 1 z to razliko, da bomo tokrat 
namesto variacijskega problema reševali optimizacijski problem. Lagrangian je potrebno zapisati v 
primerni obliki, nato pa lahko uporabimo neko metodo optimizacije, da dobimo rešitev. \\

\section{Opis reševanja}
Reševanja sem se, kot je navada, lotil v Pythonu. Za optimizacijo sem uporabil knjižnico \texttt{scipy},
ki vsebuje veliko različnih metod za optimizacijo. Poskušal sem tudi z komercialnim solverjem \texttt{gurobipy}
do katerega sem dobil dostop v kontekstu prejšnje naloge, ampak nisem dosegel željenih rezultatov. Ideja
je bila, da bi potem reševanje paraleliziral. Poleg tega sem pa uporabil praktično stalen nabor knjižnic, ki
torej \texttt{numpy}, \texttt{matplotlib}, \texttt{pandas} etc. \\

\subsection{Suite of benchmarks}
Za primerjavo uspešnosti različnih metod globalne optimizacije sem definiral suito funkcij, ki so posebej
patološke oz. primerne za testiranje kvalitete optimizacijskih metod. Funkcije, ki sem jih definiral sem 
narisal na sliki \ref{fig:benchmarks, fig:benchmarks2, fig:benchmarks3}. \\



\subsection{Thomsonov problem}
Najprej sem napisal funkcijo, ki je za dobljene kote $\theta$ in $\phi$ na enotski sferi izračunala
potencial. Pri temu sem upošteval, da imamo pravzaprav $m = n - 1$ točk, ker je ena točka fiksna. Njo
sem postavil na severni pol. Preostale točke sem porazdelil enakomerno po ekvatorju. Pogledal sem si
natančnost različnih metod, ki jih nudi \texttt{scipy.optimize.minimize()} pri $m=1$, kajti to je edini
primer za katerega sem se počutil samozavestno, da poznam analitičen odgovor. To pomeni, da sta dva naboja 
na krogli, kar pomeni, da gre ne fiksirani naboj v drug pol. \\



\section{Rezultati}


\section{Komentarji in izboljšave}

\newpage
\bibliographystyle{unsrt}
\bibliography{sources}
\end{document}
